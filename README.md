ğŸ“ Edu-RAG Backend

A FastAPI-based Retrieval-Augmented Generation (RAG) backend designed for educational use cases.
This service retrieves relevant context from a vector database (Qdrant) and generates AI-powered answers using OpenAI models.

ğŸš€ Live API

ğŸ”— Backend URL (Render):
https://edu-rag-backend.onrender.com

ğŸ“Œ Features

ğŸ” Semantic search using vector embeddings

ğŸ§  Retrieval-Augmented Generation (RAG)

ğŸ“„ PDF-based knowledge ingestion

âš¡ FastAPI REST API

â˜ï¸ Cloud-hosted on Render

ğŸ” Secure API keys via environment variables

ğŸŒ CORS-enabled for frontend integration

ğŸ§± Tech Stack
Backend

Python 3.10+

FastAPI

OpenAI API (Embeddings + Generation)

Qdrant Cloud (Vector Database)

Uvicorn (ASGI Server)

ML / NLP

Sentence Transformers

OpenAI Embeddings

Chunk-based document indexing

ğŸ—‚ï¸ Project Structure
.
â”œâ”€â”€ main.py                 # FastAPI app & /query endpoint
â”œâ”€â”€ index_pdfs.py           # PDF ingestion & indexing script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_utils.py        # PDF text extraction
â”‚   â”œâ”€â”€ rag_utils.py        # Chunking & answer generation
â”‚   â””â”€â”€ qdrant_utils.py     # Qdrant setup & upserts
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ”„ How the RAG Pipeline Works
1ï¸âƒ£ Indexing (Offline Step)

PDFs are read from a folder

Text is chunked into smaller sections

Each chunk is converted into embeddings

Embeddings are stored in Qdrant

2ï¸âƒ£ Querying (Runtime)

User sends a question to /query

Question is embedded

Qdrant retrieves top-K relevant chunks

Retrieved context is passed to an AI model

Final answer is returned to the client

ğŸ“¡ API Endpoints
Health Check
GET /


Response

{
  "status": "ok"
}

Ask a Question (RAG)
POST /query


Request Body

{
  "question": "What is machine learning?",
  "top_k": 5
}


Response

{
  "answer": "Machine learning is a subset of artificial intelligence..."
}

âš™ï¸ Environment Variables

Create these in Render or a .env file locally:

OPENAI_API_KEY=your_openai_key
QDRANT_URL=https://your-qdrant-url
QDRANT_API_KEY=your-qdrant-api-key
COLLECTION_NAME=chatbot_with_qdrant
PDF_FOLDER=data


âš ï¸ Never commit API keys to GitHub

ğŸ› ï¸ Running Locally
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Index PDFs (one-time step)
python index_pdfs.py

3ï¸âƒ£ Start the server
uvicorn main:app --reload


Server will run at:

http://127.0.0.1:8000

ğŸŒ Deployment

Platform: Render

Start Command

uvicorn main:app --host 0.0.0.0 --port $PORT


Auto-deploy: Enabled via GitHub

Free tier: Spins down after inactivity (cold start delay expected)

ğŸŒ CORS Configuration

Backend allows requests from:

http://localhost:3000

https://eduragai.netlify.app

This enables secure frontendâ€“backend communication.

âš ï¸ Notes & Limitations

Free OpenAI tier may hit rate limits

Cold starts on Render may cause first request delay

AI responses may be inaccurate â€” verify critical info

ğŸ§  Use Cases

Educational chatbots

AI tutoring systems

Legal / academic document Q&A

RAG experimentation projects

ğŸ‘¨â€ğŸ’» Author

Dhruv Buge
Computer Science (AI & ML) Undergraduate
GitHub: https://github.com/Dhruvbuge

ğŸ“„ License

This project is intended for educational and learning purposes.
